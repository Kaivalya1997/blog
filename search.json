[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Mathematics & AI",
    "section": "",
    "text": "Welcome to My Blog! üéØ\n\n\n\nHi there! I‚Äôm Kaivalya Dabhadkar, and this is my digital playground where I share my adventures through the fascinating worlds of mathematics and artificial intelligence.\n\n\nI‚Äôve discovered something magical in my learning journey: intuition comes before rigor. Too often, academic texts dive straight into formal proofs and definitions, leaving us wondering: - ‚ÄúWait, why did someone think to do it this way?‚Äù ü§î - ‚ÄúIf I were discovering this for the first time, how would I have approached it?‚Äù üí≠ - ‚ÄúWhat‚Äôs the story behind this seemingly arbitrary choice?‚Äù üìñ\nThat‚Äôs exactly what I aim to fix here! I believe the best way to truly understand any concept is to:\n\nFeel it first - Get an intuitive sense of what‚Äôs happening\nQuestion everything - Ask the ‚Äúwhy‚Äù questions that textbooks often skip\nTeach to learn - Feynman Techniquing my way one at a time :)\n\n\n\n\nMy primary passion lies in Mathematics and Artificial Intelligence, but I love exploring any topic that sparks curiosity. Think of this blog as a conversation with a friend who‚Äôs just as excited about understanding the ‚Äúwhy‚Äù behind the ‚Äúwhat.‚Äù\n\n\nüìê Mathematics: Deep dives into pure and applied mathematics, from linear algebra to topology\nü§ñ AI & Machine Learning: Explorations of neural networks, deep learning architectures, and AI theory\nüìä Signal Processing: Understanding signals, systems, and their transformations \n\n\n\n\n\n\nStatistical Learning Theory\nVariational Autoencoders"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Exploring Mathematics & AI",
    "section": "Recent Posts",
    "text": "Recent Posts\nBrowse through my latest articles below, or use the category filters to find topics that interest you most."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "All Posts",
    "section": "",
    "text": "Explore all blog posts below. Use the category filters and search to find specific topics."
  },
  {
    "objectID": "posts.html#browse-all-articles",
    "href": "posts.html#browse-all-articles",
    "title": "All Posts",
    "section": "",
    "text": "Explore all blog posts below. Use the category filters and search to find specific topics."
  },
  {
    "objectID": "posts/clt-intuitive-derivation/index.html",
    "href": "posts/clt-intuitive-derivation/index.html",
    "title": "The Bell Curve rising: An Intuitive CLT Derivation",
    "section": "",
    "text": "The Central Limit Theorem (CLT) answers an important question: Why does the bell curve (or Normal Distribution) show up everywhere in the real world?\n\n\n\nThe Normal Distribution, showing the 68-95-99.7 rule.\n\n\nSpecifically, the theorem describes what happens when you take a random variable, \\(X\\), and repeat the experiment many times to get a series of outcomes, \\(X_1, X_2, \\dots, X_m\\). What does the distribution of their sum (\\(S_m = X_1 + \\dots + X_m\\)) or their average (\\(\\bar{X}_m = S_m/m\\)) look like when \\(m\\) is very large?\nFor the magic to happen, the random variables must be IID (Independent and Identically Distributed).\n\nIndependent: The outcome of one trial doesn‚Äôt influence the next (the die has no memory).\nIdentically Distributed: We‚Äôre drawing from the same random process every time (e.g., rolling the same weird die).\n\nWhen these conditions are met, the Central Limit Theorem states that the distribution of the sum (or average) will converge to a perfect bell curve.\nBut what makes this so profound is its universality. It doesn‚Äôt matter what the probability distribution of the original variable \\(X\\) was. You could be rolling a standard six-sided die, flipping a biased coin, or measuring something with a bizarre, skewed distribution that looks nothing like a bell curve. The result is always the same: after summing enough of them, the bell curve emerges.\nWe see this everywhere:\n\nHuman Height: The height of a single person is determined by a vast number of random factors: genetics, nutrition, environment, and so on. Each factor contributes a small amount, pushing the final height up or down. While the distribution of each individual factor is unknown and certainly not normal, their sum results in the classic bell-curve distribution of heights we see in the population.\nManufacturing: The number of microscopic flaws in a single microchip might follow some random, unknown distribution. But the total number of flaws in a batch of 500 chips will, again, follow a bell curve, which is the foundation of statistical quality control.\n\nThe individual randomness gets ‚Äúaveraged out,‚Äù and a predictable, universal shape emerges. The final bell curve only cares about the mean and variance of the original die, completely forgetting all other details of its shape. This derivation is a mathematical journey to see exactly why this phenomenon occurs."
  },
  {
    "objectID": "posts/clt-intuitive-derivation/index.html#the-clt",
    "href": "posts/clt-intuitive-derivation/index.html#the-clt",
    "title": "The Bell Curve rising: An Intuitive CLT Derivation",
    "section": "",
    "text": "The Central Limit Theorem (CLT) answers an important question: Why does the bell curve (or Normal Distribution) show up everywhere in the real world?\n\n\n\nThe Normal Distribution, showing the 68-95-99.7 rule.\n\n\nSpecifically, the theorem describes what happens when you take a random variable, \\(X\\), and repeat the experiment many times to get a series of outcomes, \\(X_1, X_2, \\dots, X_m\\). What does the distribution of their sum (\\(S_m = X_1 + \\dots + X_m\\)) or their average (\\(\\bar{X}_m = S_m/m\\)) look like when \\(m\\) is very large?\nFor the magic to happen, the random variables must be IID (Independent and Identically Distributed).\n\nIndependent: The outcome of one trial doesn‚Äôt influence the next (the die has no memory).\nIdentically Distributed: We‚Äôre drawing from the same random process every time (e.g., rolling the same weird die).\n\nWhen these conditions are met, the Central Limit Theorem states that the distribution of the sum (or average) will converge to a perfect bell curve.\nBut what makes this so profound is its universality. It doesn‚Äôt matter what the probability distribution of the original variable \\(X\\) was. You could be rolling a standard six-sided die, flipping a biased coin, or measuring something with a bizarre, skewed distribution that looks nothing like a bell curve. The result is always the same: after summing enough of them, the bell curve emerges.\nWe see this everywhere:\n\nHuman Height: The height of a single person is determined by a vast number of random factors: genetics, nutrition, environment, and so on. Each factor contributes a small amount, pushing the final height up or down. While the distribution of each individual factor is unknown and certainly not normal, their sum results in the classic bell-curve distribution of heights we see in the population.\nManufacturing: The number of microscopic flaws in a single microchip might follow some random, unknown distribution. But the total number of flaws in a batch of 500 chips will, again, follow a bell curve, which is the foundation of statistical quality control.\n\nThe individual randomness gets ‚Äúaveraged out,‚Äù and a predictable, universal shape emerges. The final bell curve only cares about the mean and variance of the original die, completely forgetting all other details of its shape. This derivation is a mathematical journey to see exactly why this phenomenon occurs."
  },
  {
    "objectID": "posts/clt-intuitive-derivation/index.html#encoding-probabilities-the-pgf",
    "href": "posts/clt-intuitive-derivation/index.html#encoding-probabilities-the-pgf",
    "title": "The Bell Curve rising: An Intuitive CLT Derivation",
    "section": "Encoding Probabilities (The PGF)",
    "text": "Encoding Probabilities (The PGF)\nFirst, we need a way to mathematically describe our single die. We‚Äôll package its probabilities into a special polynomial called a Probability Generating Function (PGF). This isn‚Äôt just a convenient trick; it‚Äôs a cornerstone of combinatorics for solving counting problems.\n\nA gentler start: probabilities as polynomials\nImagine you want to count the number of ways to form 10 Rupees using a collection of 1, 2, and 5 Rupee coins. This is a classic combinatorial problem. A powerful way to solve it is to represent the available coins as polynomials:\n\n1 Rupee Coins: \\((1 + x^1 + x^2 + \\dots)\\)\n2 Rupee Coins: \\((1 + x^2 + x^4 + \\dots)\\)\n5 Rupee Coins: \\((1 + x^5 + x^{10} + \\dots)\\)\n\nWhen you multiply these polynomials, voila! The coefficient of \\(x^{10}\\) in the final product gives you the exact number of ways to make change for 10 Rupees.\nWhy does this work? The exponents add, just like the coin values do. The polynomial multiplication automatically explores every single combination of choices for you.\nA PGF does the exact same thing, but for probabilities. For a discrete random variable \\(X\\) that takes integer values with probabilities \\(p_k=\\Pr(X=k)\\), we define: \\[H(z)=\\sum_k p_k\\,z^k.\\]\nWhy is this natural?\n\nLabels, not powers: The exponent \\(k\\) is just a label for the outcome \\(X=k\\). The polynomial is a fancy storage system where the coefficient of \\(z^k\\) holds the probability of the outcome \\(k\\).\nConvolution by multiplication: This is the killer feature. If \\(X\\) and \\(Y\\) are independent, the probability that \\(X+Y=n\\) is found by summing over all pairs of outcomes that add to \\(n\\): \\(\\sum_k \\Pr(X=k)\\Pr(Y=n-k)\\). This operation is called a convolution. When you multiply the PGFs \\(H_X(z)\\) and \\(H_Y(z)\\), the rule for multiplying polynomials does exactly the same calculation. The coefficient of \\(z^n\\) in the product is precisely that sum! So, for sums of independent variables, the PGF of the sum is the product of the PGFs: \\[H_{X+Y}(z)=H_X(z)\\,H_Y(z).\\]\nFrom coins to dice: For a biased coin where heads \\(=1\\) and tails \\(=0\\), with \\(\\Pr(1)=p\\), the PGF is \\(H(z) = (1-p)z^0 + pz^1\\). For two flips, the PGF is \\(H(z)^2 = (1-p)^2 + 2p(1-p)z^1 + p^2z^2\\). The coefficients are the binomial probabilities! This idea scales perfectly to dice or any other discrete distribution.\n\nFor distributions that can take negative integer values, the PGF becomes a Laurent series with negative powers, like \\(H(z) = \\sum_{k=-\\infty}^{\\infty} p_k z^k\\). All the logic, including the convolution property and the Cauchy integral extractor, works exactly the same!\nFor a standard fair die, the outcomes are \\(\\{1, 2, 3, 4, 5, 6\\}\\), each with probability \\(\\tfrac{1}{6}\\). The blueprint function is: \\[\nh(z) = \\frac{1}{6}z^1 + \\frac{1}{6}z^2 + \\frac{1}{6}z^3 + \\frac{1}{6}z^4 + \\frac{1}{6}z^5 + \\frac{1}{6}z^6\n\\]\n\n\nThe Magic of Multiplication\nHere‚Äôs where the magic happens. What if we want the probabilities for the sum of two dice? We simply multiply the blueprint by itself: \\(h(z)^2\\).\nWhy does this work? Consider getting a total of 3. This can happen as (1+2) or (2+1). When we multiply out \\(h(z) \\times h(z)\\), the math automatically finds every combination that produces a \\(z^3\\) term: \\[\n\\underbrace{\\left(\\tfrac{1}{6}z^1\\right) \\times \\left(\\tfrac{1}{6}z^2\\right)}_{\\text{Roll 1, then 2}} +\n\\underbrace{\\left(\\tfrac{1}{6}z^2\\right) \\times \\left(\\tfrac{1}{6}z^1\\right)}_{\\text{Roll 2, then 1}}\n= \\frac{2}{36}z^3\n\\]\nThe math does all the tedious bookkeeping for us! The coefficient \\(\\tfrac{2}{36}\\) is exactly the correct probability for the sum being 3.\nThis powerful idea is our starting point: To find the probability that the sum of \\(m\\) rolls is \\(n\\), we just need to find the coefficient of the \\(z^n\\) term in the expansion of \\(h(z)^m\\). We write this as \\([z^n]h(z)^m\\).\n\nA worked example (three dice)\nLet \\(h(z)=\\tfrac{1}{6}(z+z^2+z^3+z^4+z^5+z^6)\\). For three dice, \\[\n[z^9]h(z)^3 = \\sum_{a+b+c=9} \\tfrac{1}{6^3}\\,1 = \\frac{10}{216},\n\\] where the \\(10\\) solutions \\((a,b,c)\\in\\{1,\\dots,6\\}^3\\) with \\(a+b+c=9\\) are counted by simple stars-and-bars with bounds (or by direct enumeration). This matches the usual table of sums for three dice.\n\n\nNotation: the coefficient operator\nFor a formal power series \\(F(z)=\\sum_n c_n z^n\\), the extractor is \\[[z^n]F(z)=c_n.\\] When \\(F\\) is analytic on and inside a circle \\(\\gamma\\), Cauchy‚Äôs formula gives an integral representation of this operator (next section)."
  },
  {
    "objectID": "posts/clt-intuitive-derivation/index.html#the-coefficient-extractor-machine",
    "href": "posts/clt-intuitive-derivation/index.html#the-coefficient-extractor-machine",
    "title": "The Bell Curve rising: An Intuitive CLT Derivation",
    "section": "The ‚ÄúCoefficient Extractor‚Äù Machine",
    "text": "The ‚ÄúCoefficient Extractor‚Äù Machine\n\nWhy do we need an extractor?\nFor a small number of dice, say \\(m=3\\), we could grit our teeth and manually expand the polynomial \\(h(z)^3\\) to find the probability of any given sum. But for \\(m=100\\), this is impossible. The polynomial would have hundreds of terms, and we only care about one of them! We need a surgical tool, a mathematical machine that can reach into the enormous expansion of \\(h(z)^m\\) and pull out just the single coefficient we need, \\([z^n]h(z)^m\\), without doing the whole expansion.\n\n\nHow to Isolate a Coefficient (The Residue Trick)\nLet‚Äôs think about a general polynomial \\(P(z) = c_0 + c_1 z + c_2 z^2 + \\dots\\). How could we isolate, say, \\(c_n\\)?\nA brilliant trick from complex analysis involves division. If we divide \\(P(z)\\) by \\(z^{n+1}\\), we get: \\[\n\\frac{P(z)}{z^{n+1}} = \\frac{c_0}{z^{n+1}} + \\frac{c_1}{z^n} + \\dots + \\frac{c_n}{z} + c_{n+1} + c_{n+2}z + \\dots\n\\] Look at that! The coefficient we want, \\(c_n\\), is now attached to the \\(\\frac{1}{z}\\) term. This special term has a name: it‚Äôs the residue of the function at \\(z=0\\).\nThis is where the magic happens. The Cauchy‚Äôs Residue Theorem provides our extractor machine. It states that if you integrate a complex function around a closed loop, the result is exactly \\(2\\pi i\\) times the sum of the residues of the poles inside that loop. In our case, with the pole at \\(z=0\\), we get: \\[\n\\oint_\\gamma \\frac{P(z)}{z^{n+1}} dz = 2\\pi i \\cdot (\\text{residue at } z=0) = 2\\pi i \\cdot c_n\n\\] Rearranging this gives us our formula! This is a specialized version known as Cauchy‚Äôs integral formula for coefficients: \\[\nc_n = [z^n]P(z) = \\frac{1}{2\\pi i} \\oint_\\gamma \\frac{P(z)}{z^{n+1}} dz\n\\] This is our perfect machine. It works even if \\(P(z)\\) is a Laurent series with negative powers, allowing us to handle random variables with both positive and negative outcomes. We can now find any coefficient just by performing an integral, completely bypassing the nightmarish polynomial expansion.\n\n\nThe Machine in Action (and a Surprising Transformation!)\nLet‚Äôs use the simplest and most convenient loop: the unit circle, parametrized by \\(z = e^{i\\theta}\\) for \\(\\theta \\in [-\\pi, \\pi]\\). Let‚Äôs see what happens to our formula.\n\n\n\nA diagram of the unit circle in the complex plane, showing \\(z = e^{i\\theta}\\).\n\n\n\nThe function is \\(f(z) = h(z)^m\\).\nThe differential is \\(dz = i e^{i\\theta} d\\theta\\).\nThe denominator is \\(z^{n+1} = (e^{i\\theta})^{n+1} = e^{i(n+1)\\theta}\\).\n\nPlugging these in, we get: \\[\n\\Pr(S_m=n) = \\frac{1}{2\\pi i} \\int_{-\\pi}^{\\pi} \\frac{h(e^{i\\theta})^m}{e^{i(n+1)\\theta}} (i e^{i\\theta} d\\theta)\n\\] The \\(i\\) in the numerator cancels the \\(i\\) in the denominator. The \\(e^{i\\theta}\\) from \\(dz\\) cancels one power in the denominator, changing \\(e^{i(n+1)\\theta}\\) to \\(e^{in\\theta}\\). The expression simplifies to: \\[\n\\Pr(S_m=n) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} h(e^{i\\theta})^m e^{-in\\theta} d\\theta\n\\] Wait a minute‚Ä¶ that‚Äôs amazing! This is exactly the formula for the \\(n\\)-th coefficient of a Fourier series!\n\n\nThe Deep Connection: Power Series and Fourier Series\nWhy did this happen? This is not a coincidence, it‚Äôs one of the most beautiful and profound connections in mathematics. It‚Äôs about changing your point of view.\n\nA Power Series (like our PGF) is built on a basis of monomials: \\(\\{1, z, z^2, z^3, \\dots\\}\\). These are simple algebraic objects.\nA Fourier Series is built on a basis of complex exponentials: \\(\\{\\dots, e^{-2i\\theta}, e^{-i\\theta}, 1, e^{i\\theta}, e^{2i\\theta}, \\dots\\}\\). These are functions of pure oscillation.\n\nWhen we evaluate our PGF on the unit circle by setting \\(z=e^{i\\theta}\\), we are literally swapping out the algebraic basis for the oscillation basis. The power \\(z^k\\) becomes the pure frequency \\(e^{ik\\theta}\\).\nThe integral in Cauchy‚Äôs formula is a machine for extracting coefficients by averaging over a circular path. The integral in Fourier analysis is a machine for extracting coefficients by correlating with a specific frequency (projecting onto a basis function). It turns out they are the same machine! The act of choosing the unit circle as our path in the complex plane forces the coefficient-extractor to behave just like a Fourier component-extractor.\nThe PGF is the natural tool from the world of combinatorics and algebra. The Fourier series is the natural tool from the world of waves, signals, and harmonic analysis. The Central Limit Theorem lives at the intersection of these two worlds, and the unit circle is the portal between them. This is why the Fourier transform emerging here isn‚Äôt just natural, it‚Äôs inevitable."
  },
  {
    "objectID": "posts/clt-intuitive-derivation/index.html#analyzing-the-integral-with-a-taylor-series",
    "href": "posts/clt-intuitive-derivation/index.html#analyzing-the-integral-with-a-taylor-series",
    "title": "The Bell Curve rising: An Intuitive CLT Derivation",
    "section": "Analyzing the Integral with a Taylor Series",
    "text": "Analyzing the Integral with a Taylor Series\nThis integral is exact, but to solve it, we must approximate it for large \\(m\\).\n\nThe ‚ÄúCharacteristic Function‚Äù Appears Out of Nowhere!\nA natural question is: why not expand \\(h(e^{i\\theta})^m\\) directly? A function raised to a large power \\(m\\) is mathematically very difficult to handle. The standard and most powerful technique is to first analyze its logarithm. This clever step turns the difficult power \\(m\\) into a simple multiplier.\nSo let‚Äôs look closely at the thing we‚Äôre taking the log of: \\(h(e^{i\\theta})\\). What is it, really? Let‚Äôs write it out from its definition: \\[\nh(e^{i\\theta}) = \\sum_k \\Pr(X=k) e^{ik\\theta}\n\\] This is a sum where each possible outcome‚Äôs complex exponential \\(e^{ik\\theta}\\) is weighted by its probability. This is just the expected value of the complex random variable \\(e^{i\\theta X}\\)! \\[\nh(e^{i\\theta}) = \\mathbb{E}[e^{i\\theta X}]\n\\] This object is so important in probability theory that it has its own name: the Characteristic Function, denoted \\(\\varphi_X(\\theta)\\).\nThis is a key moment. In many academic books, the characteristic function is introduced on page one like a rabbit pulled from a hat, leaving you wondering where it came from. But here, it hasn‚Äôt been pulled from a hat at all! It has emerged completely naturally. We started with the intuitive idea of a PGF for counting. We then used a powerful complex analysis machine to extract a coefficient. By running that machine on the unit circle, the PGF automatically transformed itself into the characteristic function. It‚Äôs the Fourier transform of the probability distribution, and it appears because we chose a Fourier-like method to solve our problem. Amazing!\nSo, we define \\(g(\\theta) = \\log \\varphi_X(\\theta) = \\log h(e^{i\\theta})\\). Our integrand‚Äôs exponent is now \\(m\\, g(\\theta) - in\\theta\\).\n\n\nThe Taylor Series and Cumulants\nThe natural next step to approximate the integral is to expand the function \\(g(\\theta)\\) as a Taylor series. This immediately raises the crucial question: around which point should we center the expansion? As we will see, the entire success of the approximation hinges on choosing \\(\\theta=0\\).\nThe rigorous answer for this choice lies in analyzing the magnitude of our integrand. The integral we must solve is \\(\\frac{1}{2\\pi} \\int \\varphi_X(\\theta)^m e^{-in\\theta} d\\theta\\). The dominant term that dictates the behavior of the integral for large \\(m\\) is \\(\\varphi_X(\\theta)^m\\). To analyze its magnitude, we use a key property of complex numbers: the magnitude of a number raised to a power is the magnitude of the number raised to that same power, i.e., \\(|z^n|=|z|^n\\). Therefore, the magnitude of our term is simply \\(|\\varphi_X(\\theta)|^m\\).\nOur first step is to prove that the magnitude of the characteristic function, \\(|\\varphi_X(\\theta)|\\), is at most 1. The intuition for this comes from visualizing the sum of complex numbers as vector addition‚Äîsince complex numbers add component-wise, just like vectors, the process can be seen as joining vectors tip-to-tail. The triangle inequality (\\(|\\sum z_i| \\le \\sum |z_i|\\)) simply states that the length of the final resulting vector can never be greater than the sum of the lengths of all the individual vectors.\nBy definition, \\(\\varphi_X(\\theta) = \\sum_k p_k e^{ik\\theta}\\). Applying the triangle inequality: \\[\n||\\varphi_X(\\theta)| = \\left|\\sum_k p_k e^{ik\\theta}\\right| \\le \\sum_k |p_k e^{ik\\theta}| = \\sum_k p_k |e^{ik\\theta}|\n\\] Since \\(|e^{ik\\theta}|=1\\) for any real \\(k\\) and \\(\\theta\\), this simplifies to: \\[\n||\\varphi_X(\\theta)| \\le \\sum_k p_k = 1\n\\] The equality \\(|\\varphi_X(\\theta)|=1\\) holds if and only if all the complex numbers \\(e^{ik\\theta}\\) (for which \\(p_k&gt;0\\)) point in the same direction. For any non-trivial distribution (with at least two different outcomes), this only happens when \\(\\theta=0\\). At \\(\\theta=0\\), every term \\(e^{ik\\cdot 0}\\) is just 1. For any \\(\\theta \\neq 0\\), the different values of \\(k\\) cause the terms to have different phases, so they are no longer perfectly aligned, and the magnitude of their sum is strictly less than 1. This means that the function \\(|\\varphi_X(\\theta)|\\) has a unique, global maximum value of 1 at precisely \\(\\theta=0\\).\nNow, consider what happens when we raise this to a large power, \\(m\\). The peak at \\(\\theta=0\\), where the value is \\(1^m = 1\\), remains. However, for any other value of \\(\\theta\\) where \\(|\\varphi_X(\\theta)| &lt; 1\\), the value of \\(|\\varphi_X(\\theta)|^m\\) plummets towards zero exponentially fast as \\(m\\) grows. For instance, if at some point the magnitude is 0.99, for \\(m=1000\\) it becomes \\(0.99^{1000} \\approx 4 \\times 10^{-5}\\).\nThis creates an extremely sharp ‚Äúspike‚Äù in the integrand‚Äôs magnitude, centered at \\(\\theta=0\\). The result is that the only part of the integral that contributes significantly to the final value comes from an infinitesimally small neighborhood around \\(\\theta=0\\). The contributions from all other regions are exponentially suppressed and become negligible. Therefore, to accurately approximate the integral for large \\(m\\), we must approximate the function \\(g(\\theta)\\) in this tiny, all-important region around the origin. The Taylor series is the fundamental mathematical tool for this. Expanding around \\(\\theta=0\\) is not a mere convenience; it is a mathematically necessary step.\nNow that we have rigorously established why we must expand around \\(\\theta=0\\), we can proceed. The Taylor series of \\(g(\\theta) = \\log \\mathbb{E}[e^{i\\theta X}]\\) is special because its coefficients define the cumulants (\\(\\kappa_j\\)) of our distribution. The general formula for the \\(j\\)-th cumulant is given by the \\(j\\)-th derivative of the log-generating function evaluated at the origin: \\[\n\\kappa_j = \\frac{1}{i^j} \\left. \\frac{d^j}{d\\theta^j} \\, g(\\theta) \\right|_{\\theta=0}.\n\\]\nThese cumulants are the distribution‚Äôs fundamental properties:\n\n\\(\\kappa_1 = \\mu\\) (the mean)\n\\(\\kappa_2 = \\sigma^2\\) (the variance)\n\\(\\kappa_3 = \\mathbb{E}[(X-\\mu)^3]\\) (the third central moment, a measure of skewness)\n\\(\\kappa_4 = \\mathbb{E}[(X-\\mu)^4] - 3(\\sigma^2)^2\\) (related to kurtosis)\n\nThe Taylor series of \\(g(\\theta)\\) around \\(\\theta=0\\) (assuming the corresponding moments exist) is therefore:\n\\[\ng(\\theta) = i\\mu\\,\\theta - \\frac{\\sigma^2}{2}\\,\\theta^2 - i\\,\\frac{\\kappa_3}{6}\\,\\theta^3 + \\frac{\\kappa_4}{24}\\,\\theta^4 + O(\\theta^5).\n\\]"
  },
  {
    "objectID": "posts/clt-intuitive-derivation/index.html#the-bell-curve-emerges",
    "href": "posts/clt-intuitive-derivation/index.html#the-bell-curve-emerges",
    "title": "The Bell Curve rising: An Intuitive CLT Derivation",
    "section": "The Bell Curve Emerges",
    "text": "The Bell Curve Emerges\n\nSetting the Stage\nNow we put our Taylor series approximation back into the integral for \\(\\Pr(S_m=n)\\). The exponent in our integral, \\(m\\, g(\\theta) - in\\theta\\), becomes: \\[\nm\\left(i\\mu\\,\\theta - \\frac{\\sigma^2}{2}\\,\\theta^2 - i\\frac{\\kappa_3}{6}\\,\\theta^3 + \\frac{\\kappa_4}{24}\\,\\theta^4 + \\dots\\right) - in\\theta.\n\\] This still looks like a mess. The key insight is that for large \\(m\\), the sum \\(S_m\\) will almost certainly be very close to its expected value, \\(m\\mu\\). We are interested in the shape of the distribution around this mean. The question ‚Äúwhat does the distribution of the sum look like?‚Äù is really ‚Äúwhat does the distribution of deviations from the mean look like?‚Äù\nTo formalize this, we need a new coordinate system that is centered at the mean and scaled appropriately. The mean of the sum \\(S_m\\) is \\(m\\mu\\), and its variance is \\(m\\sigma^2\\), making its standard deviation \\(\\sigma\\sqrt{m}\\). It is natural to measure deviations in units of this standard deviation.\nSo, we define a new variable \\(x\\) that does exactly this: \\[\nn = m\\mu + x\\sigma\\sqrt{m}\n\\] Here, \\(x\\) is our new coordinate, representing the number of standard deviations from the mean. An \\(x\\) value of 0 is the mean, \\(x=1\\) is one standard deviation above the mean, and so on. This change of variables is not just a trick; it is the fundamental act of standardizing the random variable so we can compare the shapes of distributions that have different means and variances.\nLet‚Äôs substitute this into our exponent: \\[\nm\\left(i\\mu\\,\\theta - \\frac{\\sigma^2}{2}\\,\\theta^2 - \\dots\\right) - i(m\\mu + x\\sigma\\sqrt{m})\\theta\n\\] Distributing the last term gives: \\[\n(im\\mu\\theta - m\\frac{\\sigma^2}{2}\\theta^2 - \\dots) - im\\mu\\theta - ix\\sigma\\sqrt{m}\\theta\n\\] Look! The main term involving the mean, \\(im\\mu\\theta\\), cancels out perfectly! This is no accident; this is why we centered our coordinate system at the mean. The purpose of this step was to eliminate the distracting shifting effect of the mean so we could focus purely on the shape of the distribution. The exponent becomes: \\[\n-m\\frac{\\sigma^2}{2}\\theta^2 - ix\\sigma\\sqrt{m}\\theta - m\\left(i\\frac{\\kappa_3}{6}\\theta^3 + \\dots\\right)\n\\]\n\n\nThe Mathematical Microscope\nOur exponent, \\(-m\\frac{\\sigma^2}{2}\\theta^2 - ix\\sigma\\sqrt{m}\\theta - m\\left(i\\frac{\\kappa_3}{6}\\theta^3 + \\dots\\right)\\), still looks complicated. The key to simplifying it is to understand the behavior for large \\(m\\).\nThe term \\(e^{-m\\frac{\\sigma^2}{2}\\theta^2}\\) acts as a Gaussian ‚Äúenvelope‚Äù that rapidly decays to zero as \\(\\theta\\) moves away from the origin. As \\(m\\) gets larger, this envelope shrinks tighter and tighter, meaning the effective region of integration gets narrower. This provides a formal reason for our earlier insight: the integral is dominated by the behavior near \\(\\theta=0\\).\nThis observation forces a critical question: how do we properly ‚Äúzoom in‚Äù on the origin to see the stable, non-trivial behavior of the integral as \\(m \\to \\infty\\)? We need a change of variables for \\(\\theta\\) that depends on \\(m\\). Let‚Äôs consider the dominant term, \\(m\\theta^2\\). For the integral to converge to a meaningful, constant shape, this term must be stable‚Äîit cannot vanish or diverge as \\(m \\to \\infty\\). This implies that \\(m\\theta^2\\) must be of order 1. This simple requirement dictates the scaling law: \\[\nm\\theta^2 \\sim \\text{constant} \\implies \\theta \\sim \\frac{1}{\\sqrt{m}}\n\\] This is the unique scaling that works. If we zoomed in too much (e.g., \\(\\theta \\sim 1/m\\)), the quadratic term would vanish and we‚Äôd lose the Gaussian shape. If we didn‚Äôt zoom in enough (e.g., \\(\\theta \\sim 1/m^{1/4}\\)), the term would blow up.\nThis insight motivates the formal change of variables: \\[\n\\theta = \\frac{u}{\\sqrt{m}}\n\\] Here, \\(u\\) is our new coordinate, a ‚Äústabilized‚Äù variable that lives on a constant scale while we send \\(m \\to \\infty\\). This change of variables is like placing the integrand under a mathematical microscope with a zoom level of \\(\\sqrt{m}\\), perfectly calibrated to reveal the universal shape hidden at the origin. It is important to remember that this is a standard change of variables for a definite integral. We are simply re-scaling our axis of integration; the value of the integral is preserved as long as we correctly transform the differential (\\(d\\theta = du/\\sqrt{m}\\)) and the integration limits. Let‚Äôs see why this is perfect by examining each term in the exponent:\n\nThe Variance Term: \\[\n-m\\frac{\\sigma^2}{2}\\theta^2 \\quad \\longrightarrow \\quad -m\\frac{\\sigma^2}{2}\\left(\\frac{u}{\\sqrt{m}}\\right)^2 = -m\\frac{\\sigma^2}{2}\\frac{u^2}{m} = -\\frac{\\sigma^2}{2}u^2\n\\] The \\(m\\) vanishes completely! This term becomes independent of \\(m\\), forming the stable, universal core of our bell curve. This is the main event.\nThe \\(x\\) Term (from centering): \\[\n-ix\\sigma\\sqrt{m}\\theta \\quad \\longrightarrow \\quad -ix\\sigma\\sqrt{m}\\left(\\frac{u}{\\sqrt{m}}\\right) = -ix\\sigma u\n\\] This term is also stable and independent of \\(m\\). It controls the position under the bell curve.\nThe Skewness Term (\\(\\kappa_3\\)): \\[\n-m\\left(i\\frac{\\kappa_3}{6}\\theta^3\\right) \\quad \\longrightarrow \\quad -mi\\frac{\\kappa_3}{6}\\left(\\frac{u}{\\sqrt{m}}\\right)^3 = -mi\\frac{\\kappa_3}{6}\\frac{u^3}{m^{3/2}} = -i\\frac{\\kappa_3 u^3}{6\\sqrt{m}}\n\\] This term has a \\(\\sqrt{m}\\) in the denominator, so it vanishes as \\(m \\to \\infty\\)!\nThe Kurtosis Term (\\(\\kappa_4\\)): \\[\nm\\left(\\frac{\\kappa_4}{24}\\theta^4\\right) \\quad \\longrightarrow \\quad m\\frac{\\kappa_4}{24}\\left(\\frac{u}{\\sqrt{m}}\\right)^4 = \\frac{\\kappa_4 u^4}{24 m}\n\\] This is smaller, of order \\(m^{-1}\\), and contributes to higher-order accuracy (e.g., beyond the first Edgeworth correction).\nHigher-Order Terms: All subsequent terms will have even higher powers of \\(\\sqrt{m}\\) in the denominator (\\(m, m^{3/2}, \\dots\\)), so they will disappear even faster.\n\nOur once-horrible exponent simplifies beautifully: \\[\n\\text{Exponent} = \\underbrace{-\\frac{\\sigma^2}{2}u^2 - ix\\sigma u}_{\\text{The stable Bell Curve shape}} \\; - \\; \\frac{i\\kappa_3 u^3}{6\\sqrt{m}} \\; + \\; \\frac{\\kappa_4 u^4}{24 m} \\; + \\; O\\!\\left(m^{-3/2}\\right)\n\\] As \\(m \\to \\infty\\), all the terms that depend on the specific, unique features of our original die (skewness, kurtosis, etc.) are washed away. Only the universal terms related to the mean and variance survive. This is the heart of the CLT‚Äôs universality!\nOur integral for the probability becomes: \\[\n\\Pr(S_m=n) \\approx \\frac{1}{2\\pi} \\int \\exp\\left(-\\frac{\\sigma^2 u^2}{2} - ix\\sigma u\\right) (du/\\sqrt{m})\n\\] The change of variables \\(d\\theta = du/\\sqrt{m}\\) introduces the \\(\\frac{1}{\\sqrt{m}}\\) factor. The integral is now over \\(u\\). Since the integrand shrinks to zero incredibly fast for large \\(u\\), we can extend the limits of integration from \\([-\\pi\\sqrt{m}, \\pi\\sqrt{m}]\\) to \\([-\\infty, \\infty]\\) with negligible error. \\[\n\\Pr(S_m=n) \\approx \\frac{1}{2\\pi\\sqrt{m}} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{\\sigma^2 u^2}{2} - ix\\sigma u\\right) \\, du.\n\\]\n\n\nEvaluating the Gaussian Integral and Finding the PDF\nThis is a standard Gaussian integral. We can solve it by completing the square in the exponent: \\[\n-\\frac{\\sigma^2 u^2}{2} - ix\\sigma u = -\\frac{\\sigma^2}{2}\\left(u^2 + \\frac{2ix}{\\sigma}u\\right) = -\\frac{\\sigma^2}{2}\\left(\\left(u + \\frac{ix}{\\sigma}\\right)^2 - \\left(\\frac{ix}{\\sigma}\\right)^2\\right) = -\\frac{\\sigma^2}{2}\\left(u + \\frac{ix}{\\sigma}\\right)^2 - \\frac{x^2}{2}\n\\] The integral becomes: \\[\n\\int_{-\\infty}^{\\infty} e^{-\\frac{\\sigma^2}{2}(u + ix/\\sigma)^2 - x^2/2} du = e^{-x^2/2} \\int_{-\\infty}^{\\infty} e^{-\\frac{\\sigma^2}{2}(u + ix/\\sigma)^2} du\n\\] The integral \\(\\int e^{-a(t+b)^2}dt\\) over the real line is \\(\\sqrt{\\pi/a}\\), regardless of the shift \\(b\\) (which can be proven by shifting the contour in the complex plane). Here, \\(a=\\sigma^2/2\\), so the integral evaluates to \\(\\sqrt{2\\pi/\\sigma^2}\\). Plugging this back in: \\[\n\\Pr(S_m=n) \\approx \\frac{1}{2\\pi\\sqrt{m}} \\cdot \\sqrt{\\frac{2\\pi}{\\sigma^2}} e^{-x^2/2} = \\frac{1}{\\sigma\\sqrt{2\\pi m}} e^{-x^2/2}.\n\\] This is the Local Limit Theorem. It gives the probability of a single outcome. But what does it represent? Let‚Äôs rearrange it. The quantity \\(1/(\\sigma\\sqrt{m})\\) is the spacing between possible values of our standardized variable \\(x\\). Let‚Äôs call this spacing \\(\\Delta x\\). Then \\[\n\\Pr(S_m=n) \\approx \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\cdot \\frac{1}{\\sigma\\sqrt{m}} = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\Delta x.\n\\] This is beautiful! It says that the probability of landing in a small interval of width \\(\\Delta x\\) around the value \\(x\\) is approximately the height of a curve, \\(\\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\\), times the width of the interval. This is the definition of a probability density function!\nAs \\(m\\to\\infty\\), the discrete steps \\(\\Delta x\\) become infinitesimally small, and the approximation becomes exact. We have just derived the probability density function (PDF) of the standardized normal distribution: \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n\\] The mean and variance of the original distribution are hidden in here. The mean \\(\\mu\\) was removed when we centered our variable \\(x\\). The variance \\(\\sigma^2\\) was scaled away to give this universal shape. This shows that any sum of i.i.d. variables, when recentered and rescaled, will converge to this exact same shape.\n\n\nA Sharper Approximation: The Edgeworth Correction\nWhat if we keep the next term in the exponent? \\[\ne^{\\text{Exponent}} \\approx e^{-\\frac{\\sigma^2 u^2}{2} - ix\\sigma u} \\cdot e^{-i\\frac{\\kappa_3 u^3}{6\\sqrt{m}}}\n\\] For large \\(m\\), the argument of the second exponential is small, so we can use the approximation \\(e^y \\approx 1+y\\): \\[\ne^{-\\frac{\\sigma^2 u^2}{2} - ix\\sigma u} \\left(1 - i\\frac{\\kappa_3 u^3}{6\\sqrt{m}}\\right)\n\\] Integrating this gives the main Gaussian term plus a correction term involving the integral of \\(u^3\\) against the Gaussian. The result is the first-order Edgeworth series expansion: \\[\n\\Pr(S_m=n) \\approx \\frac{1}{\\sigma\\sqrt{2\\pi m}} e^{-x^2/2}\n\\left[1 + \\frac{\\kappa_3}{6\\,\\sigma^3\\,\\sqrt{m}}(x^3-3x)\\right].\n\\] This correction term accounts for the skewness (\\(\\kappa_3\\)) of the original distribution, and it‚Äôs our first hint of how the sum approaches the bell curve, along with the shape of the errors in the approximation."
  },
  {
    "objectID": "posts/clt-intuitive-derivation/index.html#beyond-integer-valued-variables-real-valued-x",
    "href": "posts/clt-intuitive-derivation/index.html#beyond-integer-valued-variables-real-valued-x",
    "title": "The Bell Curve rising: An Intuitive CLT Derivation",
    "section": "Beyond integer-valued variables (real-valued \\(X\\))",
    "text": "Beyond integer-valued variables (real-valued \\(X\\))\nSo far, our random variable \\(X\\) could only take integer values. What if it‚Äôs a continuous variable that can take any real value? We can cleverly handle this by thinking of a continuous value as a limit of finely-grained discrete values.\nLet‚Äôs imagine a continuous random variable \\(X\\) with some probability density function. We can approximate it with a discrete variable \\(X_N\\) that takes values on a fine grid with spacing \\(1/N\\): \\(k/N\\) for integers \\(k\\). The probability that \\(X_N\\) takes the value \\(k/N\\) would be approximately the density of \\(X\\) at that point times the spacing: \\(\\Pr(X_N = k/N) \\approx f(k/N) \\cdot (1/N)\\).\nNow, we can build a PGF for this discretized variable \\(X_N\\). But the powers will be fractions, like \\(z^{k/N}\\). This looks strange, but we can make it familiar with a simple substitution. Let a new variable be \\(\\zeta = z^{1/N}\\). Then our PGF becomes a normal polynomial (or Laurent series) in \\(\\zeta\\): \\[\nH_N(\\zeta) = \\sum_k \\Pr(X_N = k/N) \\zeta^k\n\\] This is amazing! We‚Äôve transformed the continuous problem back into the discrete integer problem we just solved. We can now use our entire machinery on this new PGF.\nThe sum of \\(m\\) such independent variables, \\(S_m = X_{N,1} + \\dots + X_{N,m}\\), will have a PGF of \\(H_N(\\zeta)^m\\). The probability that this sum equals a certain value, say \\(n/N\\), is given by the coefficient of \\(\\zeta^n\\): \\[\n\\Pr(S_m = n/N) = [\\zeta^n]H_N(\\zeta)^m\n\\] We can run this through our whole derivation. The mean and variance of \\(X_N\\) will be approximately the mean and variance of \\(X\\), and as we take the limit \\(N \\to \\infty\\), they will become exact. The final result for the probability will look just like our Local Limit Theorem, but for the variable \\(\\zeta\\).\nWhen we translate back, we find that the probability density function of the sum \\(S_m\\) converges to the same bell curve shape. The key insight is that the underlying logic‚Äîconvolution becoming multiplication, logarithms linearizing powers, and the dominance of the quadratic term in the Taylor series‚Äîis completely independent of whether the original variable was discrete or continuous. By discretizing, we can see that the same universal force is at play. The passage to the continuous case is just a matter of taking the limit as our grid becomes infinitely fine. This confirms that the CLT is a truly universal law of probability."
  }
]