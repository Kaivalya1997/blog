---
title: "A Rigorous Deep Dive into a Power-Basis Cubic VAE"
subtitle: "From First Principles to a VAE Implementation: The Surprising Complexity of a Simple Idea"
author: "Kaivalya Dabhadkar"
date: "2025-09-05"
categories: [AI, Deep Learning, Variational Inference, Generative Models, Mathematics]
toc: true
number-sections: false
highlight-style: github
format:
  html:
    code-fold: true
    code-summary: "Show code"
    html-math-method: mathjax
draft: true
published: false
search: false
---

## 1. The Premise: Seeking a Simpler, More Flexible Latent Space

### Interactive VAE Visualization

```{ojs}
//| echo: false

// Import D3 for animations
d3 = require("d3@7")

// Create the comprehensive VAE visualization
{
  const width = 850;
  const height = 480;
  
  const container = d3.create("div")
    .style("display", "flex")
    .style("justify-content", "center")
    .style("align-items", "center")
    .style("width", "100%")
    .style("margin", "20px auto");
  
  const svg = container.append("svg")
    .attr("viewBox", `0 0 ${width} ${height}`)
    .attr("width", width)
    .attr("height", height)
    .style("max-width", "100%")
    .style("height", "auto")
    .style("background", "linear-gradient(135deg, #667eea 0%, #764ba2 100%)")
    .style("border-radius", "12px")
    .style("box-shadow", "0 10px 30px rgba(0,0,0,0.2)");
  
  // Add gradient definitions
  const defs = svg.append("defs");
  
  // Background gradient
  const bgGradient = defs.append("linearGradient")
    .attr("id", "bg-gradient")
    .attr("x1", "0%").attr("y1", "0%")
    .attr("x2", "100%").attr("y2", "100%");
  
  bgGradient.append("stop")
    .attr("offset", "0%")
    .style("stop-color", "#1e3c72")
    .style("stop-opacity", 0.9);
  
  bgGradient.append("stop")
    .attr("offset", "100%")
    .style("stop-color", "#2a5298")
    .style("stop-opacity", 0.9);
  
  // Apply gradient background
  svg.append("rect")
    .attr("width", width)
    .attr("height", height)
    .attr("fill", "url(#bg-gradient)");
  
  // Title with glow effect
  const title = svg.append("text")
    .attr("x", width / 2)
    .attr("y", 30)
    .attr("text-anchor", "middle")
    .style("font-size", "22px")
    .style("font-weight", "bold")
    .style("fill", "white")
    .style("filter", "drop-shadow(0 2px 4px rgba(0,0,0,0.3))")
    .text("Variational Autoencoder (VAE) Architecture");
  
  // Define positions
  const inputX = width * 0.1;
  const encoderX = width * 0.25;
  const muSigmaX = width * 0.4;
  const latentX = width * 0.55;
  const decoderX = width * 0.7;
  const outputX = width * 0.85;
  const centerY = height / 2;
  
  // Create neural network visualization for encoder
  const encoderGroup = svg.append("g").attr("opacity", 0);
  
  // Draw encoder layers
  const encoderLayers = [4, 3, 2];
  encoderLayers.forEach((neurons, layerIdx) => {
    for (let i = 0; i < neurons; i++) {
      const y = centerY - (neurons - 1) * 15 + i * 30;
      encoderGroup.append("circle")
        .attr("cx", encoderX + layerIdx * 25)
        .attr("cy", y)
        .attr("r", 6)
        .attr("fill", "#4FC3F7")
        .attr("stroke", "white")
        .attr("stroke-width", 1.5);
    }
  });
  
  encoderGroup.append("text")
    .attr("x", encoderX + 25)
    .attr("y", centerY + 60)
    .attr("text-anchor", "middle")
    .style("font-size", "12px")
    .style("fill", "white")
    .style("font-weight", "500")
    .text("Encoder Network");
  
  // Create input data visualization (as image grid)
  const inputGroup = svg.append("g");
  const gridSize = 5;
  const cellSize = 8;
  
  for (let i = 0; i < gridSize; i++) {
    for (let j = 0; j < gridSize; j++) {
      inputGroup.append("rect")
        .attr("x", inputX - gridSize * cellSize / 2 + i * cellSize)
        .attr("y", centerY - gridSize * cellSize / 2 + j * cellSize)
        .attr("width", cellSize - 1)
        .attr("height", cellSize - 1)
        .attr("fill", `hsl(${Math.random() * 60 + 200}, 70%, ${50 + Math.random() * 30}%)`)
        .attr("opacity", 0.9);
    }
  }
  
  inputGroup.append("text")
    .attr("x", inputX)
    .attr("y", centerY + 60)
    .attr("text-anchor", "middle")
    .style("font-size", "12px")
    .style("fill", "white")
    .style("font-weight", "500")
    .text("Input Data");
  
  // Mean and Variance visualization
  const muSigmaGroup = svg.append("g").attr("opacity", 0);
  
  // Mean (μ)
  muSigmaGroup.append("circle")
    .attr("cx", muSigmaX)
    .attr("cy", centerY - 25)
    .attr("r", 12)
    .attr("fill", "#FFD54F")
    .attr("stroke", "white")
    .attr("stroke-width", 2);
  
  muSigmaGroup.append("text")
    .attr("x", muSigmaX)
    .attr("y", centerY - 21)
    .attr("text-anchor", "middle")
    .style("font-size", "14px")
    .style("fill", "#333")
    .style("font-weight", "bold")
    .text("μ");
  
  // Variance (σ)
  muSigmaGroup.append("circle")
    .attr("cx", muSigmaX)
    .attr("cy", centerY + 25)
    .attr("r", 12)
    .attr("fill", "#81C784")
    .attr("stroke", "white")
    .attr("stroke-width", 2);
  
  muSigmaGroup.append("text")
    .attr("x", muSigmaX)
    .attr("y", centerY + 29)
    .attr("text-anchor", "middle")
    .style("font-size", "14px")
    .style("fill", "#333")
    .style("font-weight", "bold")
    .text("σ");
  
  muSigmaGroup.append("text")
    .attr("x", muSigmaX)
    .attr("y", centerY + 70)
    .attr("text-anchor", "middle")
    .style("font-size", "11px")
    .style("fill", "white")
    .style("font-weight", "500")
    .text("Parameters");
  
  // Latent space visualization with mixture of Gaussians
  const latentGroup = svg.append("g").attr("opacity", 0);
  
  // Draw multiple Gaussian distributions for mixture
  const gaussianCenters = [
    {x: latentX - 25, y: centerY - 15, color: "#E1BEE7"},
    {x: latentX + 20, y: centerY - 10, color: "#C5CAE9"},
    {x: latentX, y: centerY + 15, color: "#D1C4E9"}
  ];
  
  // Draw contours for each Gaussian in the mixture
  gaussianCenters.forEach((center, gIdx) => {
    const radii = [30, 22, 14];
    radii.forEach((radius, idx) => {
      latentGroup.append("ellipse")
        .attr("cx", center.x)
        .attr("cy", center.y)
        .attr("rx", radius)
        .attr("ry", radius * 0.85)
        .attr("fill", "none")
        .attr("stroke", center.color)
        .attr("stroke-width", 1)
        .attr("stroke-dasharray", idx === 0 ? "2,2" : "none")
        .attr("opacity", 0.5 - idx * 0.12);
    });
  });
  
  // Sampled points from mixture
  const sampledPoints = [];
  for (let i = 0; i < 8; i++) {
    const point = latentGroup.append("circle")
      .attr("cx", latentX)
      .attr("cy", centerY)
      .attr("r", 3)
      .attr("fill", "#FF6B6B")
      .attr("stroke", "white")
      .attr("stroke-width", 1)
      .attr("opacity", 0);
    sampledPoints.push(point);
  }
  
  // Reparameterization trick visualization with proper math
  const reparamGroup = latentGroup.append("g");
  
  // Create white background for formula
  reparamGroup.append("rect")
    .attr("x", latentX - 45)
    .attr("y", centerY - 65)
    .attr("width", 90)
    .attr("height", 28)
    .attr("rx", 4)
    .attr("fill", "rgba(255,255,255,0.95)")
    .attr("stroke", "rgba(255,255,255,0.3)")
    .attr("stroke-width", 1);
  
  // Main formula
  reparamGroup.append("text")
    .attr("x", latentX)
    .attr("y", centerY - 50)
    .attr("text-anchor", "middle")
    .style("font-size", "11px")
    .style("fill", "#1e3c72")
    .style("font-weight", "600")
    .style("font-family", "serif")
    .style("font-style", "italic")
    .text("z = μ + σ ⊙ ε");
  
  // Distribution notation
  reparamGroup.append("text")
    .attr("x", latentX)
    .attr("y", centerY - 75)
    .attr("text-anchor", "middle")
    .style("font-size", "9px")
    .style("fill", "#FFECB3")
    .style("font-family", "serif")
    .style("font-style", "italic")
    .text("ε ~ 𝒩(0,I)");
  
  latentGroup.append("text")
    .attr("x", latentX)
    .attr("y", centerY + 70)
    .attr("text-anchor", "middle")
    .style("font-size", "12px")
    .style("fill", "white")
    .style("font-weight", "500")
    .text("Latent Space (Mixture)");
  
  // Add label for mixture
  latentGroup.append("text")
    .attr("x", latentX)
    .attr("y", centerY + 85)
    .attr("text-anchor", "middle")
    .style("font-size", "9px")
    .style("fill", "#E1BEE7")
    .style("font-family", "serif")
    .style("font-style", "italic")
    .text("q(z) = Σᵢ πᵢ 𝒩(μᵢ, σᵢ²)");
  
  // Decoder network
  const decoderGroup = svg.append("g").attr("opacity", 0);
  
  // Draw decoder layers (mirror of encoder)
  const decoderLayers = [2, 3, 4];
  decoderLayers.forEach((neurons, layerIdx) => {
    for (let i = 0; i < neurons; i++) {
      const y = centerY - (neurons - 1) * 15 + i * 30;
      decoderGroup.append("circle")
        .attr("cx", decoderX + layerIdx * 25)
        .attr("cy", y)
        .attr("r", 6)
        .attr("fill", "#FF7043")
        .attr("stroke", "white")
        .attr("stroke-width", 1.5);
    }
  });
  
  decoderGroup.append("text")
    .attr("x", decoderX + 25)
    .attr("y", centerY + 60)
    .attr("text-anchor", "middle")
    .style("font-size", "12px")
    .style("fill", "white")
    .style("font-weight", "500")
    .text("Decoder Network");
  
  // Output/reconstruction visualization
  const outputGroup = svg.append("g").attr("opacity", 0);
  
  for (let i = 0; i < gridSize; i++) {
    for (let j = 0; j < gridSize; j++) {
      outputGroup.append("rect")
        .attr("x", outputX - gridSize * cellSize / 2 + i * cellSize)
        .attr("y", centerY - gridSize * cellSize / 2 + j * cellSize)
        .attr("width", cellSize - 1)
        .attr("height", cellSize - 1)
        .attr("fill", `hsl(${Math.random() * 60 + 200}, 60%, ${45 + Math.random() * 25}%)`)
        .attr("opacity", 0.8);
    }
  }
  
  outputGroup.append("text")
    .attr("x", outputX)
    .attr("y", centerY + 60)
    .attr("text-anchor", "middle")
    .style("font-size", "12px")
    .style("fill", "white")
    .style("font-weight", "500")
    .text("Reconstruction");
  
  // Loss visualization
  const lossGroup = svg.append("g").attr("opacity", 0);
  
  lossGroup.append("rect")
    .attr("x", width / 2 - 100)
    .attr("y", height - 90)
    .attr("width", 200)
    .attr("height", 50)
    .attr("rx", 8)
    .attr("fill", "rgba(255,255,255,0.1)")
    .attr("stroke", "rgba(255,255,255,0.3)")
    .attr("stroke-width", 1);
  
  lossGroup.append("text")
    .attr("x", width / 2)
    .attr("y", height - 65)
    .attr("text-anchor", "middle")
    .style("font-size", "11px")
    .style("fill", "#FFF9C4")
    .style("font-family", "serif")
    .style("font-style", "italic")
    .text("ℒ = ℒrecon + ℒKL");
  
  lossGroup.append("text")
    .attr("x", width / 2 - 50)
    .attr("y", height - 48)
    .attr("text-anchor", "middle")
    .style("font-size", "10px")
    .style("fill", "#FFCDD2")
    .style("font-family", "serif")
    .style("font-style", "italic")
    .text("‖x − x̂‖²");
  
  lossGroup.append("text")
    .attr("x", width / 2 + 50)
    .attr("y", height - 48)
    .attr("text-anchor", "middle")
    .style("font-size", "10px")
    .style("fill", "#C5E1A5")
    .style("font-family", "serif")
    .style("font-style", "italic")
    .text("𝒟KL(q‖p)");
  
  // Connection lines
  const connections = svg.append("g").attr("opacity", 0);
  
  // Define arrow markers
  defs.append("marker")
    .attr("id", "arrow-white")
    .attr("viewBox", "0 0 10 10")
    .attr("refX", 10)
    .attr("refY", 5)
    .attr("markerWidth", 6)
    .attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path")
    .attr("d", "M 0 0 L 10 5 L 0 10 z")
    .attr("fill", "rgba(255,255,255,0.8)");
  
  // Connection paths
  const pathData = [
    {x1: inputX + 25, y1: centerY, x2: encoderX - 10, y2: centerY},
    {x1: encoderX + 60, y1: centerY, x2: muSigmaX - 15, y2: centerY},
    {x1: muSigmaX + 15, y1: centerY, x2: latentX - 50, y2: centerY},
    {x1: latentX + 50, y1: centerY, x2: decoderX - 10, y2: centerY},
    {x1: decoderX + 60, y1: centerY, x2: outputX - 25, y2: centerY}
  ];
  
  pathData.forEach(d => {
    connections.append("line")
      .attr("x1", d.x1)
      .attr("y1", d.y1)
      .attr("x2", d.x2)
      .attr("y2", d.y2)
      .attr("stroke", "rgba(255,255,255,0.4)")
      .attr("stroke-width", 2)
      .attr("marker-end", "url(#arrow-white)");
  });
  
  // Play button
  const button = svg.append("g")
    .attr("cursor", "pointer")
    .attr("transform", `translate(${width/2 - 60}, 60)`);
  
  button.append("rect")
    .attr("width", 120)
    .attr("height", 35)
    .attr("rx", 18)
    .attr("fill", "rgba(255,255,255,0.9)")
    .attr("stroke", "white")
    .attr("stroke-width", 2)
    .style("filter", "drop-shadow(0 4px 6px rgba(0,0,0,0.2))");
  
  button.append("text")
    .attr("x", 60)
    .attr("y", 22)
    .attr("text-anchor", "middle")
    .style("font-size", "14px")
    .style("fill", "#1e3c72")
    .style("font-weight", "bold")
    .text("▶ Play");
  
  // Animation function
  function runAnimation() {
    // Reset everything
    encoderGroup.attr("opacity", 0);
    muSigmaGroup.attr("opacity", 0);
    latentGroup.attr("opacity", 0);
    decoderGroup.attr("opacity", 0);
    outputGroup.attr("opacity", 0);
    lossGroup.attr("opacity", 0);
    connections.attr("opacity", 0);
    
    // Phase 1: Show encoder network
    encoderGroup
      .transition()
      .delay(500)
      .duration(800)
      .attr("opacity", 1);
    
    connections
      .transition()
      .delay(500)
      .duration(800)
      .attr("opacity", 1);
    
    // Phase 2: Show μ and σ
    muSigmaGroup
      .transition()
      .delay(1500)
      .duration(800)
      .attr("opacity", 1);
    
    // Phase 3: Show latent space with sampling animation
    latentGroup
      .transition()
      .delay(2500)
      .duration(800)
      .attr("opacity", 1);
    
    // Animate sampling points from mixture
    sampledPoints.forEach((point, idx) => {
      const gaussian = gaussianCenters[idx % 3];
      point
        .attr("opacity", 0)
        .transition()
        .delay(3300 + idx * 100)
        .duration(300)
        .attr("opacity", 0.9)
        .attr("cx", gaussian.x + (Math.random() - 0.5) * 20)
        .attr("cy", gaussian.y + (Math.random() - 0.5) * 18)
        .transition()
        .duration(400)
        .attr("r", 4)
        .transition()
        .duration(300)
        .attr("r", 3);
    });
    
    // Phase 4: Show decoder
    decoderGroup
      .transition()
      .delay(4000)
      .duration(800)
      .attr("opacity", 1);
    
    // Phase 5: Show reconstruction
    outputGroup
      .transition()
      .delay(5000)
      .duration(800)
      .attr("opacity", 1);
    
    // Phase 6: Show loss
    lossGroup
      .transition()
      .delay(6000)
      .duration(800)
      .attr("opacity", 1);
  }
  
  // Add click handler
  button.on("click", runAnimation);
  
  // Auto-run animation on load
  setTimeout(runAnimation, 800);
  
  yield container.node();
}

md`
**This animation illustrates the complete VAE process:**

1. **📥 Encoding**: Input data (shown as pixel grid) is processed through the encoder neural network
2. **📊 Parameters**: The encoder outputs mean (μ) and variance (σ) parameters
3. **🎲 Reparameterization**: Using the formula z = μ + σ ⊙ ε where ε ~ 𝒩(0,I)
4. **🌐 Latent Space**: Shows a mixture of Gaussians distribution with multiple modes
5. **📤 Decoding**: Sampled points are decoded through the decoder network
6. **🖼️ Reconstruction**: Output attempts to reconstruct the original input
7. **📉 Loss Function**: ℒ = ℒrecon + ℒKL balances reconstruction quality with regularization

The visualization demonstrates how VAEs can learn complex, multimodal latent distributions while maintaining differentiability through the reparameterization trick.

*Click "▶ Play" to replay the animation.*
`
```

### 1.1. The Reign of the Gaussian VAE

In the vast landscape of generative models, the Variational Autoencoder (VAE) stands as a cornerstone, celebrated for its elegant fusion of deep learning and Bayesian inference. At its heart lies a simple yet profound idea: learn a compressed, probabilistic representation of data in a lower-dimensional latent space. The default, near-universal choice for modeling this latent space, for both the prior $p(\mathbf{z})$ and the approximate posterior $q_\phi(\mathbf{z}|\mathbf{x})$, is the Gaussian distribution.

This dominance is not accidental. The Gaussian offers two critical conveniences that make the entire VAE framework computationally tractable:

1.  **The Reparameterization Trick**: This technique allows us to sample from the distribution in a way that keeps the computation graph fully differentiable. For a Gaussian, it's as simple as $ \mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon} $, where $\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$. This moves the stochasticity "outside" the parameters we need to learn, enabling low-variance gradients to flow freely.
2.  **An Analytical KL Divergence**: The Evidence Lower Bound (ELBO), the objective function for VAEs, contains a Kullback–Leibler (KL) divergence term that measures the "distance" between the posterior and the prior. When both are Gaussian, this term can be calculated in a simple, closed-form expression, avoiding the need for noisy Monte Carlo estimation.

While convenient, this "Gaussian-by-default" approach imposes a rigid structure on the latent space that can be fundamentally at odds with the data's true nature. The limitations are significant:

-   **Unbounded Support**: The Gaussian is defined over all real numbers $(-\infty, \infty)$. If the underlying factors of variation in our data are naturally bounded—such as representing angles, proportions, probabilities, or physical attributes that cannot be negative—the Gaussian is a structurally inappropriate model.
-   **Unimodality**: A simple diagonal Gaussian has only one peak, or mode. It cannot efficiently represent data that has several distinct clusters or modes in its latent representation. The model is forced to "average" over these modes, often resulting in blurry, unrealistic samples and reconstructions.

### 1.2. The Quest for a Simple Alternative: The Cubic Polynomial

This leads us to a foundational question: can we find a distribution that is nearly as simple as the Gaussian but offers greater flexibility? The most natural step up in complexity from a quadratic function (which defines the log-probability of a Gaussian) is a **cubic polynomial**.

Our goal in this post is to embark on a deep, rigorous, and exhaustive exploration of this seemingly straightforward idea. We will attempt to build a VAE latent distribution from the ground up using a standard, power-basis cubic polynomial:
$$
p(t) = a_3 t^3 + a_2 t^2 + a_1 t + a_0
$$
To make our analysis concrete, we will define this distribution over the symmetric, bounded interval $[-1, 1]$. We aim for our final model to have **three tunable parameters** that a neural network can learn, creating a direct analogy to the two tunable parameters (mean and standard deviation) of the Gaussian.

This post will be a detailed mathematical journey. We will derive, from first principles, the precise and surprisingly complex constraints that the coefficients $(a_0, a_1, a_2, a_3)$ must satisfy for $p(t)$ to be a valid probability density function. We will discover that enforcing these constraints within a neural network is a formidable challenge, leading us to a crucial insight about the nature of model design. This exploration will ultimately reveal why alternative approaches, such as using a different polynomial basis, are not just a matter of preference but a necessary step for creating a robust and practical model.

---

## 2. The Labyrinth of Constraints: Deriving a Power-Basis Cubic PDF

For our polynomial $p(t)$ to serve as a valid Probability Density Function (PDF) on the interval $[-1, 1]$, it must satisfy two non-negotiable axioms of probability theory:

1.  **Normalization**: The total probability over the entire sample space must equal one.
    $$ \int_{-1}^{1} p(t) dt = 1 $$
2.  **Non-negativity**: The probability density must be non-negative everywhere within the sample space.
    $$ p(t) \ge 0 \quad \text{for all} \quad t \in [-1, 1] $$

While these axioms seem simple, applying them to a power-basis polynomial unveils a web of interconnected constraints.

### 2.1. The Normalization Constraint: A First Simplification

Let's begin by enforcing the normalization axiom. We integrate our polynomial $p(t) = a_3 t^3 + a_2 t^2 + a_1 t + a_0$ over the symmetric interval $[-1, 1]$:
$$
\int_{-1}^{1} (a_3 t^3 + a_2 t^2 + a_1 t + a_0) dt = 1
$$
A key property of definite integrals is that any odd function (where $f(-x) = -f(x)$) integrated over a symmetric interval is zero. In our polynomial, the terms $a_3 t^3$ and $a_1 t$ are odd functions.
$$
\int_{-1}^{1} (a_3 t^3 + a_1 t) dt = \left[ \frac{a_3}{4}t^4 + \frac{a_1}{2}t^2 \right]_{-1}^{1} = \left(\frac{a_3}{4} + \frac{a_1}{2}\right) - \left(\frac{a_3(-1)^4}{4} + \frac{a_1(-1)^2}{2}\right) = 0
$$
The even function terms, $a_2 t^2$ and $a_0$, survive. Their integral over a symmetric interval is twice the integral over the positive half.
$$
\int_{-1}^{1} (a_2 t^2 + a_0) dt = 2 \int_{0}^{1} (a_2 t^2 + a_0) dt = 2 \left[ \frac{a_2}{3}t^3 + a_0 t \right]_{0}^{1} = 2\left(\frac{a_2}{3} + a_0\right)
$$
Setting this result to 1 gives us our first major constraint on the coefficients:
$$
2\left(\frac{a_2}{3} + a_0\right) = 1 \quad \implies \quad \boxed{a_0 = \frac{1}{2} - \frac{a_2}{3}}
$$
This is a powerful result. The normalization requirement has eliminated one of our four degrees of freedom. This means we are left with exactly **three free parameters**—$a_1, a_2, a_3$—which a neural network can learn. This aligns perfectly with our initial goal. Our normalized cubic PDF now takes the form:
$$
p(t) = a_3 t^3 + a_2 t^2 + a_1 t + \left(\frac{1}{2} - \frac{a_2}{3}\right)
$$

### 2.2. The Non-Negativity Constraint: The True Challenge

This is where the apparent simplicity of the cubic polynomial reveals its hidden complexity. We must guarantee that $p(t) \ge 0$ for **every** point $t \in [-1, 1]$. For a continuous function on a closed interval, this is equivalent to ensuring that its value at the boundaries and at any internal local minima is non-negative.

#### Step 1: Boundary Conditions

First, we evaluate $p(t)$ at the interval boundaries, $t=1$ and $t=-1$, and require them to be non-negative.
For $t=1$:
$$
p(1) = a_3(1)^3 + a_2(1)^2 + a_1(1) + \left(\frac{1}{2} - \frac{a_2}{3}\right) = a_3 + a_1 + \frac{2a_2}{3} + \frac{1}{2} \ge 0
$$
For $t=-1$:
$$
p(-1) = a_3(-1)^3 + a_2(-1)^2 + a_1(-1) + \left(\frac{1}{2} - \frac{a_2}{3}\right) = -a_3 - a_1 + \frac{2a_2}{3} + \frac{1}{2} \ge 0
$$
These two linear inequalities can be combined. Adding them together gives a simple bound on $a_2$:
$$
(p(1) + p(-1)) = \left(a_3 + a_1 + \frac{2a_2}{3} + \frac{1}{2}\right) + \left(-a_3 - a_1 + \frac{2a_2}{3} + \frac{1}{2}\right) = \frac{4a_2}{3} + 1 \ge 0 \implies \boxed{a_2 \ge -\frac{3}{4}}
$$
This also gives us a condition on the combination of $a_1$ and $a_3$. From the two inequalities, we must have:
$$
a_3 + a_1 \ge -\left(\frac{2a_2}{3} + \frac{1}{2}\right) \quad \text{and} \quad -(a_3 + a_1) \ge -\left(\frac{2a_2}{3} + \frac{1}{2}\right)
$$
This is equivalent to:
$$
\boxed{|a_3 + a_1| \le \frac{2a_2}{3} + \frac{1}{2}}
$$
So far, these constraints, while interconnected, are still manageable.

#### Step 2: Internal Extrema Conditions

The most difficult part of the problem arises from the internal minima. A local minimum can only occur where the derivative of the function is zero. Let's find the derivative of our PDF:
$$
p'(t) = \frac{d}{dt} \left(a_3 t^3 + a_2 t^2 + a_1 t + a_0\right) = 3a_3 t^2 + 2a_2 t + a_1
$$
We must find the roots of $p'(t)=0$, which is a standard quadratic equation. The locations of the extrema are given by the quadratic formula:
$$
t^{*}_{\pm} = \frac{-2a_2 \pm \sqrt{(2a_2)^2 - 4(3a_3)(a_1)}}{2(3a_3)} = \frac{-a_2 \pm \sqrt{a_2^2 - 3a_1 a_3}}{3a_3}
$$
The behavior depends on the discriminant, $\Delta = a_2^2 - 3a_1 a_3$.

-   **Case 1: $\Delta < 0$**. If the discriminant is negative, there are no real roots. The derivative $p'(t)$ is never zero, meaning the function $p(t)$ is monotonic over the entire interval. In this case, its minimum value must be at one of the boundaries, $t=-1$ or $t=1$. Since we already handled the boundary conditions, no further work is needed. This case holds if $a_2^2 < 3a_1 a_3$.

-   **Case 2: $\Delta \ge 0$**. If the discriminant is non-negative, there are one or two real roots. For each root $t^*$ that is a local minimum, we have a new check:
    1.  Does this minimum lie **inside** our interval, i.e., $t^* \in (-1, 1)$?
    2.  If it does, we must enforce the new, non-linear constraint $p(t^*) \ge 0$.

Let's imagine the root $t^* = \frac{-a_2 + \sqrt{\Delta}}{3a_3}$ is a local minimum inside $(-1, 1)$. We would need to enforce:
$$
a_3 (t^*)^3 + a_2 (t^*)^2 + a_1 t^* + \left(\frac{1}{2} - \frac{a_2}{3}\right) \ge 0
$$
Substituting the complex expression for $t^*$ into this inequality results in a deeply complex, non-linear constraint on the parameters $a_1, a_2, a_3$. It involves square roots and polynomials of the parameters themselves, defining a bizarrely shaped valid region in the 3D parameter space.

### 2.3. Conclusion: The Impracticality of the Power Basis

To use this PDF in a VAE, an encoder would need to output three unconstrained real numbers, say from a linear layer. We would then need a differentiable function that maps these three numbers into the valid, constrained space of $(a_1, a_2, a_3)$.

The constraints we derived are:
1.  $a_2 \ge -\frac{3}{4}$
2.  $|a_3 + a_1| \le \frac{2a_2}{3} + \frac{1}{2}$
3.  A highly complex, conditional, non-linear constraint involving $p(t^*) \ge 0$ that only applies when $a_2^2 \ge 3a_1 a_3$ and when the resulting $t^*$ falls within $(-1, 1)$.

Finding a closed-form, differentiable mapping that enforces this entire set of conditions is, for all practical purposes, impossible. The "simple" power-basis polynomial has led us to an intractable parameterization problem. This is a crucial realization: **the simplicity of a model's definition does not imply simplicity of its use in optimization.**

This deep dive motivates the search for a better way. Instead of starting with a simple form and deriving complex constraints, what if we started with simple constraints and derived a more complex form? This is the philosophy that leads us to the Bernstein basis.

---

## 3. A Constructive Solution: The Bernstein Basis

The analysis in Part 1 reveals a crucial insight: ensuring the non-negativity of a polynomial in power-basis form is mathematically complex. The solution is not to wrestle with these intractable constraints, but to choose a different basis for our polynomial—one where the non-negativity is guaranteed by construction.

This is precisely what **Bernstein basis polynomials** offer. For a degree-3 polynomial on the interval $[0, 1]$, the basis functions are:
$$
\begin{aligned}
B_0(t) &= \binom{3}{0} t^0 (1-t)^3 = (1-t)^3 \\
B_1(t) &= \binom{3}{1} t^1 (1-t)^2 = 3t(1-t)^2 \\
B_2(t) &= \binom{3}{2} t^2 (1-t)^1 = 3t^2(1-t) \\
B_3(t) &= \binom{3}{3} t^3 (1-t)^0 = t^3
\end{aligned}
$$
These functions are all **non-negative** for $t \in [0, 1]$. This property is the key.

We can now define our PDF as a linear combination of these basis functions:
$$
p(t;\mathbf{c}) = \sum_{k=0}^3 c_k B_k(t)
$$
If we enforce the simple constraint that all coefficients $c_k \ge 0$, the non-negativity of $p(t)$ is automatically guaranteed.

### 3.1. Normalization of the Bernstein-Cubic

For the normalization, we integrate each basis function over $[0, 1]$. This integral is a classic result related to the Beta function, $\int_0^1 t^x (1-t)^y dt = \frac{x!y!}{(x+y+1)!}$.
$$
\int_0^1 B_k(t)\,dt = \binom{3}{k} \int_0^1 t^k (1-t)^{3-k} dt = \frac{3!}{k!(3-k)!} \frac{k!(3-k)!}{(3+1)!} = \frac{3!}{4!} = \frac{1}{4}
$$
Therefore, the integral of our PDF is:
$$
\int_0^1 p(t;\mathbf{c})\,dt = \sum_{k=0}^3 c_k \int_0^1 B_k(t)\,dt = \frac{1}{4}\sum_{k=0}^3 c_k
$$
To make this equal 1, we only need to enforce $\sum_{k=0}^3 c_k = 4$.

So, our two simple, elegant constraints are:
$$
c_k \ge 0 \quad \text{and} \quad \sum_{k=0}^3 c_k = 4
$$
A neural network can easily produce parameters satisfying these constraints by outputting a 4-dimensional vector $\boldsymbol{\alpha}$ and setting $\mathbf{c} = 4 \cdot \text{softmax}(\boldsymbol{\alpha})$. This gives us **3 effective tunable parameters**, as one is fixed by the sum constraint.

### 3.2. CDF, Sampling, and Gradients

The path forward is now clear and practical:
-   **CDF**: The Cumulative Distribution Function is $F(t;\mathbf{c}) = \sum_{k=0}^3 c_k I_k(t)$, where $I_k(t)$ are the simple polynomial antiderivatives of $B_k(t)$.
-   **Sampling**: We can find a sample $t$ by numerically solving $F(t;\mathbf{c}) = u$ for a uniform random variable $u \sim \mathrm{Uniform}(0,1)$. Robust methods like Newton's method combined with bisection are effective, as the PDF $p(t) = F'(t)$ is guaranteed non-negative.
-   **Gradients**: The pathwise gradient for reparameterization, $\frac{\partial t}{\partial c_r}$, is found using the implicit function theorem, resulting in a clean, analytical expression for backpropagation.

By switching to the Bernstein basis, we have transformed an intractable problem into an elegant, practical, and robust framework.

---

## 4. The VAE Framework with a Bernstein-Cubic Latent

We now formalize the VAE using our robust Bernstein-cubic distribution. We will define it on $[0, 1]$ for simplicity, but it can be affinely transformed to any interval $[L, U]$.

-   **Encoder & Posterior**: The encoder network $E_\phi(\mathbf{x})$ outputs a vector $\boldsymbol{\alpha}_q \in \mathbb{R}^4$ for each latent dimension. The posterior coefficients are $\mathbf{c}_q = 4 \cdot \text{softmax}(\boldsymbol{\alpha}_q)$.
-   **Prior**: The prior $p_\psi(\mathbf{z})$ is defined by a set of learnable parameters $\boldsymbol{\alpha}_p \in \mathbb{R}^4$ for each latent dimension, with $\mathbf{c}_p = 4 \cdot \text{softmax}(\boldsymbol{\alpha}_p)$.

### 4.1. The ELBO in Full Detail

The Evidence Lower Bound (ELBO) is:
$$
\mathcal{L}(\phi, \psi, \theta) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p_\psi(\mathbf{z}))
$$

#### Why the KL-divergence is intractable

The KL-divergence between the posterior and prior is:
$$
\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p_\psi(\mathbf{z})) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{q_\phi(\mathbf{z}|\mathbf{x})}{p_\psi(\mathbf{z})} \right]
$$

For our Bernstein-cubic distributions with factorized dimensions, this becomes:
$$
\text{KL} = \sum_{j=1}^d \mathbb{E}_{q_{\phi,j}(z_j|\mathbf{x})} \left[ \log \frac{q_{\phi,j}(z_j|\mathbf{x})}{p_{\psi,j}(z_j)} \right]
$$

For each dimension $j$, where both distributions are Bernstein-cubic polynomials:
$$
q_{\phi,j}(z_j|\mathbf{x}) = \sum_{k=0}^3 c^{(q)}_{j,k}(\mathbf{x}) B_k(t_j), \quad p_{\psi,j}(z_j) = \sum_{k=0}^3 c^{(p)}_{j,k} B_k(t_j)
$$

The KL-divergence per dimension requires computing:
$$
\text{KL}_j = \int_0^1 \left(\sum_{k=0}^3 c^{(q)}_{j,k} B_k(t)\right) \log \frac{\sum_{k=0}^3 c^{(q)}_{j,k} B_k(t)}{\sum_{k=0}^3 c^{(p)}_{j,k} B_k(t)} dt
$$

This integral has no closed-form solution because:
1. The logarithm of a ratio of cubic polynomials is not a polynomial
2. The product of a cubic polynomial with the log of a ratio of cubics creates a highly complex integrand
3. Unlike the Gaussian case (where both distributions are exponential-quadratic), there's no algebraic simplification available

Therefore, we must resort to the Monte Carlo formulation:
$$
\mathcal{L} \approx \frac{1}{S} \sum_{s=1}^S \left( \log p_\theta(\mathbf{x}|\mathbf{z}^{(s)}) + \log p_\psi(\mathbf{z}^{(s)}) - \log q_\phi(\mathbf{z}^{(s)}|\mathbf{x}) \right)
$$
where $\mathbf{z}^{(s)}$ is a sample from the posterior $q_\phi(\mathbf{z}|\mathbf{x})$ obtained via inverse-CDF sampling.

Each term is now fully differentiable:
-   $\log p_\theta(\mathbf{x}|\mathbf{z})$: The standard reconstruction loss. Gradients flow through the decoder and back to the sample $\mathbf{z}$.
-   $\log p_\psi(\mathbf{z})$: The log-probability of the sample under the prior. Gradients flow to the prior's parameters $\boldsymbol{\alpha}_p$.
-   $\log q_\phi(\mathbf{z}|\mathbf{x})$: The log-probability under the posterior.

The total gradient for the encoder parameters $\boldsymbol{\alpha}_q$ has two components:
1.  **Pathwise Gradient**: From the $\log p_\theta$ and $\log p_\psi$ terms, flowing "through" the sample $\mathbf{z}$ via the implicit function theorem.
2.  **Score Function Gradient**: From the $-\log q_\phi$ term, which depends directly on $\boldsymbol{\alpha}_q$.

### 4.2. ELBO: Step-by-step derivation and per-dimension form

We consider a factorized latent across dimensions $j=1,\dots,d$:
$$
q_\phi(\mathbf{z}|\mathbf{x})=\prod_{j=1}^d q_{\phi,j}(z_j|\mathbf{x}),\quad p_\psi(\mathbf{z})=\prod_{j=1}^d p_{\psi,j}(z_j).
$$
For each dimension defined on $[L_j,U_j]$, introduce the affine transform $t_j=(z_j-L_j)/(U_j-L_j)\in[0,1]$. The posterior and prior densities are
$$
q_{\phi,j}(z_j|\mathbf{x})=\frac{1}{U_j-L_j}\sum_{k=0}^3 c^{(q)}_{j,k}(\mathbf{x})\,B_k(t_j),\quad
p_{\psi,j}(z_j)=\frac{1}{U_j-L_j}\sum_{k=0}^3 c^{(p)}_{j,k}\,B_k(t_j),
$$
with coefficients constrained via softmax: $\mathbf{c}^{(q)}_{j}=4\,\text{softmax}(\boldsymbol{\alpha}^{(q)}_j(\mathbf{x}))$, $\mathbf{c}^{(p)}_{j}=4\,\text{softmax}(\boldsymbol{\alpha}^{(p)}_j)$.

The ELBO for a single datum $\mathbf{x}$ is
$$
\mathcal{L}(\mathbf{x})=\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]\;-
\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\Big[\log q_\phi(\mathbf{z}|\mathbf{x})-\log p_\psi(\mathbf{z})\Big].
$$
Using Monte Carlo with $S$ samples $\mathbf{z}^{(s)}\sim q_\phi(\cdot|\mathbf{x})$:
$$
\widehat{\mathcal{L}}(\mathbf{x})=\frac{1}{S}\sum_{s=1}^S \Big(\log p_\theta(\mathbf{x}|\mathbf{z}^{(s)})+\sum_{j=1}^d \log p_{\psi,j}(z^{(s)}_j)-\sum_{j=1}^d \log q_{\phi,j}(z^{(s)}_j|\mathbf{x})\Big).
$$
Explicitly, for each dimension $j$:
$$
\log q_{\phi,j}(z_j|\mathbf{x})=\log\Big(\sum_{k=0}^3 c^{(q)}_{j,k}(\mathbf{x})\,B_k(t_j)\Big)-\log(U_j-L_j),
$$
$$
\log p_{\psi,j}(z_j)=\log\Big(\sum_{k=0}^3 c^{(p)}_{j,k}\,B_k(t_j)\Big)-\log(U_j-L_j).
$$
The $-\log(U_j-L_j)$ terms cancel in the KL difference per dimension, yielding a numerically nicer objective.

#### Pathwise sampling and gradients
For each $j$, draw $u_j\sim\text{Uniform}(0,1)$ and solve $F_{q,j}(t_j;\mathbf{c}^{(q)}_j(\mathbf{x}))=u_j$ for $t_j\in[0,1]$, then set $z_j=L_j+(U_j-L_j)\,t_j$.
Implicit differentiation of $F_{q,j}(t_j;\mathbf{c}^{(q)}_j)=u_j$ gives
$$
\frac{\partial t_j}{\partial c^{(q)}_{j,r}}=-\,\frac{I_r(t_j)}{\sum_{k=0}^3 c^{(q)}_{j,k}\,B_k(t_j)}.
$$
Gradients w.r.t. prior coefficients are direct:
$$
\frac{\partial}{\partial c^{(p)}_{j,r}}\log p_{\psi,j}(z_j)=\frac{B_r(t_j)}{\sum_{k=0}^3 c^{(p)}_{j,k}\,B_k(t_j)}.
$$
Direct gradients for the posterior log term are
$$
\frac{\partial}{\partial c^{(q)}_{j,r}}\big(-\log q_{\phi,j}(z_j|\mathbf{x})\big)=-\,\frac{B_r(t_j)}{\sum_{k=0}^3 c^{(q)}_{j,k}\,B_k(t_j)}.
$$
Pathwise gradients propagate through $t_j$ and $z_j$ into the reconstruction $\log p_\theta$ and prior $\log p_\psi$. If $c=4\,\text{softmax}(\alpha)$, then $\partial c_s/\partial\alpha_r=4\,s_s(\delta_{sr}-s_r)$.

#### Interval choice and invariances
- Changing support from $[0,1]$ to $[L_j,U_j]$ adds a constant $-\log(U_j-L_j)$ to both $\log q$ and $\log p$; it cancels in the $\mathrm{KL}$.
- Using the same $[L_j,U_j]$ for $q$ and $p$ per dimension avoids support mismatch.

#### Numerical stability
Use $\log(\sum c_k B_k(t)+\varepsilon)$ with small $\varepsilon$, and clip Newton steps with bracketing to ensure monotone CDF inversion.

---

## 5. PyTorch Implementation

This implementation demonstrates the complete pipeline, including the custom autograd function for differentiable inverse-CDF sampling.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

def bernstein3(t):
    """Computes the Bernstein basis polynomials of degree 3."""
    B0 = (1.0 - t)**3
    B1 = 3.0 * t * (1.0 - t)**2
    B2 = 3.0 * t**2 * (1.0 - t)
    B3 = t**3
    return torch.stack([B0, B1, B2, B3], dim=-1)

def I_bernstein3(t):
    """Computes the antiderivatives of the Bernstein basis polynomials."""
    I0 = t - 1.5*t**2 + t**3 - 0.25*t**4
    I1 = 1.5*t**2 - 2.0*t**3 + 0.75*t**4
    I2 = t**3 - 0.75*t**4
    I3 = 0.25*t**4
    return torch.stack([I0, I1, I2, I3], dim=-1)

def softmax_to_c(alpha):
    """Converts unconstrained parameters to valid Bernstein coefficients."""
    return 4.0 * F.softmax(alpha, dim=-1)

def cubic_pdf01(t, alpha, eps=1e-12):
    """Computes the cubic PDF on [0,1]."""
    c = softmax_to_c(alpha)
    B = bernstein3(t)
    return (c * B).sum(dim=-1).clamp_min(eps)

def cubic_cdf01(t, alpha):
    """Computes the cubic CDF on [0,1]."""
    c = softmax_to_c(alpha)
    I = I_bernstein3(t)
    return (c * I).sum(dim=-1)

@torch.no_grad()
def inv_cdf01_solver(u, alpha, iters=12, eps=1e-12):
    """Robust numerical solver for the inverse CDF."""
    t = u.clone()
    lo, hi = torch.zeros_like(u), torch.ones_like(u)
    for _ in range(iters):
        Fval = cubic_cdf01(t, alpha)
        pdf  = cubic_pdf01(t, alpha, eps=eps)
        diff = Fval - u
        newton = (diff / pdf).clamp(-0.2, 0.2)
        t_new = (t - newton).clamp(0.0, 1.0)
        mask = diff > 0
        hi = torch.where(mask, t, hi)
        lo = torch.where(~mask, t, lo)
        escaped = (t_new < lo) | (t_new > hi)
        t = torch.where(escaped, 0.5 * (lo + hi), t_new)
    return t

class InverseCDFBernstein3(torch.autograd.Function):
    @staticmethod
    def forward(ctx, u, alpha):
        t = inv_cdf01_solver(u, alpha)
        ctx.save_for_backward(t, alpha)
        return t

    @staticmethod
    def backward(ctx, grad_out):
        t, alpha = ctx.saved_tensors
        B = bernstein3(t)
        I = I_bernstein3(t)
        c = softmax_to_c(alpha)
        pdf = (c * B).sum(dim=-1).clamp_min(1e-12)
        dt_dc = -I / pdf.unsqueeze(-1)
        s = c / 4.0
        v = dt_dc * (4.0 * s)
        v_sum = v.sum(dim=-1, keepdim=True)
        dtdalpha = v - s * v_sum
        grad_alpha = grad_out.unsqueeze(-1) * dtdalpha
        return None, grad_alpha

def sample_q01(alpha):
    """Differentiably sample from the cubic distribution on [0,1]."""
    shape = alpha.shape[:-1]
    u = torch.rand(shape, device=alpha.device)
    return InverseCDFBernstein3.apply(u, alpha)

class Encoder(nn.Module):
    def __init__(self, x_dim, h_dim, z_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(x_dim, h_dim), nn.ReLU(),
            nn.Linear(h_dim, z_dim * 4)
        )
    def forward(self, x):
        return self.net(x).view(x.size(0), -1, 4)

class Decoder(nn.Module):
    def __init__(self, z_dim, h_dim, x_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(z_dim, h_dim), nn.ReLU(),
            nn.Linear(h_dim, x_dim)
        )
    def forward(self, z):
        return self.net(z)

class CubicVAE(nn.Module):
    def __init__(self, x_dim, h_dim, z_dim, L=0.0, U=1.0):
        super().__init__()
        self.enc = Encoder(x_dim, h_dim, z_dim)
        self.dec = Decoder(z_dim, h_dim, x_dim)
        self.z_dim = z_dim
        self.L = L
        self.U = U
        self.alpha_p = nn.Parameter(torch.zeros(z_dim, 4))

    def forward(self, x, S=1):
        B = x.size(0)
        alpha_q = self.enc(x)
        elbo = 0.0
        for _ in range(S):
            t = sample_q01(alpha_q)
            z = self.L + (self.U - self.L) * t
            logits = self.dec(z)
            log_px = -F.binary_cross_entropy_with_logits(
                logits, x, reduction='none'
            ).sum(dim=1)
            log_pz = self.log_pdf(z, self.alpha_p.expand(B, -1, -1)).sum(dim=1)
            log_qz = self.log_pdf(z, alpha_q).sum(dim=1)
            elbo = elbo + (log_px + log_pz - log_qz)
        return elbo / S

    def log_pdf(self, z, alpha):
        t = (z - self.L) / (self.U - self.L)
        log_p01 = torch.log(cubic_pdf01(t, alpha))
        return log_p01 - torch.log(torch.tensor(self.U - self.L, device=z.device))
```

---

## 6. Conclusion: From a Simple Idea to a Rigorous Solution

Our investigation started with a simple and intuitive goal: to replace the Gaussian latent distribution in a VAE with a power-basis cubic polynomial. However, our rigorous mathematical analysis revealed that this "simple" idea is fraught with complexity. The non-negativity constraint imposes a set of intractable, non-linear conditions on the polynomial's coefficients, making it unsuitable for direct parameterization by a neural network.

This journey highlights a critical lesson in theoretical deep learning: an approach's utility is defined not by the simplicity of its initial form, but by the simplicity of its constraints and the ease of its integration into a gradient-based optimization pipeline.

The Bernstein basis, while appearing more complex at first glance, provides a far more elegant and practical solution. By building the non-negativity and normalization properties directly into the formulation, it allows us to define a flexible, bounded, and fully differentiable latent distribution. This demonstrates the power of choosing the right mathematical tools to turn a complex problem into a tractable one.

#### Antiderivatives used in the CDF
For completeness, the antiderivatives $I_k(t)=\int_0^t B_k(u)\,du$ are
$$
\begin{aligned}
I_0(t)&=t-\tfrac{3}{2}t^2+t^3-\tfrac{1}{4}t^4,\\
I_1(t)&=\tfrac{3}{2}t^2-2t^3+\tfrac{3}{4}t^4,\\
I_2(t)&=t^3-\tfrac{3}{4}t^4,\\
I_3(t)&=\tfrac{1}{4}t^4.
\end{aligned}
$$
Thus $F(t;\mathbf{c})=\sum_{k=0}^3 c_k I_k(t)$ and $F'(t;\mathbf{c})=\sum_{k=0}^3 c_k B_k(t)$.

---

## 7. Further Reading: Beyond Simple Posteriors

While our Bernstein-cubic VAE provides a flexible, bounded alternative to the Gaussian, the quest for more expressive posterior distributions is a rich area of research. Here are some of the most effective methods for moving beyond simple unimodal posteriors:

### Normalizing Flows

Normalizing flows are a powerful class of methods that transform a simple base distribution (like a Gaussian) into a much more complex one through a series of invertible, differentiable mappings.

- **Foundational Idea**: [Variational Inference with Normalizing Flows](https://arxiv.org/abs/1505.05770) (Rezende & Mohamed, 2015) introduces planar and radial flows.
- **Inverse Autoregressive Flow (IAF)**: A highly effective and popular choice for VAEs due to its fast sampling. [Improved VI with IAF](https://arxiv.org/abs/1606.04934) (Kingma et al., 2016).
- **Coupling and Autoregressive Flows**: These offer great stability and capacity.
  - [RealNVP](https://arxiv.org/abs/1605.08803) (Dinh et al., 2017)
  - [Masked Autoregressive Flow (MAF)](https://arxiv.org/abs/1705.07057) (Papamakarios et al., 2017)
  - [Glow](https://arxiv.org/abs/1807.03039) (Kingma & Dhariwal, 2018)
- **Spline Flows**: Use monotonic splines for a great trade-off between flexibility and stability. Excellent for bounded supports. [Neural Spline Flows](https://arxiv.org/abs/1906.04032) (Durkan et al., 2019).
- **Advanced Flows**: For even greater expressivity.
  - [Sylvester Flows](https://arxiv.org/abs/1803.05649) (Berg et al., 2018)
  - [Householder Flow](https://arxiv.org/abs/1611.09630) (Tomczak & Welling, 2016)

### Mixture Models and Hierarchical Methods

These approaches directly model multimodality or enrich the posterior through a hierarchical structure.

- **Mixture Posteriors**: Use a mixture of simple distributions (e.g., Gaussians) to form a complex posterior.
  - [Boosting VI](https://arxiv.org/abs/1611.05559) (Guo et al., 2016)
  - [Variational Boosting](https://arxiv.org/abs/1611.06585) (Miller et al., 2017)
- **Semi-Implicit Variational Inference (SIVI)**: Captures complex posteriors by mixing an explicit distribution with an implicit one. [SIVI](https://arxiv.org/abs/1805.11142) (Yin & Zhou, 2018).
- **Hierarchical/Auxiliary Posteriors**: Introduce auxiliary latent variables to create a richer posterior structure.
  - [Auxiliary Deep Generative Models](https://arxiv.org/abs/1602.05473) (Maaløe et al., 2016)
  - [Ladder VAE](https://arxiv.org/abs/1602.02282) (Sønderby et al., 2016)
  - [Hierarchical Variational Models](https://arxiv.org/abs/1602.06725) (Ranganath et al., 2016)

### Alternative Objectives and Formulations

- **Implicit Posteriors**: Use adversarial training to learn an implicit posterior without a closed-form density. [Adversarial Variational Bayes](https://arxiv.org/abs/1701.04722) (Mescheder et al., 2017).
- **Tighter Evidence Bounds**: Improve training by using a tighter lower bound on the evidence, which often leads to learning a better posterior. The canonical example is the **Importance Weighted Autoencoder (IWAE)**. [IWAE](https://arxiv.org/abs/1509.00519) (Burda et al., 2016).
